
### [原文](https://my.oschina.net/u/3585447/blog/4818143)

> 支持 gRPC 长链接，深度解读 Nacos 2.0 架构设计及新模型
>
#  Nacos 1.X架构存在的问题

## 1、 心跳数量多，导致TPS居高不下

通过心跳续约，当服务规模上升时，特别是类似Dubbo的接口级服务较多时，心跳及配置元数据的轮询数量众多，
导致集群TPS很高，系统资源高度空耗。

## 2、 通过心跳续约感知服务变化，时延长

心跳续约需要达到超时时间才会移除并通知订阅者，默认为15s，时延较长，时效性差。
若改短超时时间，当网络抖动时，会频繁触发变更推送，对客户端服务端都有更大损耗。

## 3、 UDP推送不可靠，导致QPS居高不下

由于UDP不可靠，因此客户端测需要每隔一段时间进行对账查询，保证客户端缓存的服务列表的状态正确，
当订阅客户端规模上升时，集群QPS很高，但大多数服务列表其实不会频繁改变，造成无效查询，从而存在资源空耗。

## 4、基于HTTP短连接模型，TIME_WAIT状态连接过多

HTTP短连接模型，每次客户端请求都会创建和销毁TCP链接，TCP协议销毁的链接状态是WAIT_TIME，
完全释放还需要一定时间，当TPS和QPS较高时，服务端和客户端可能有大量的WAIT_TIME状态链接，
从而会导致connect time out错误或者Cannot assign requested address 的问题。

## 5、配置模块的30秒长轮询 引起的频繁GC

配置模块使用HTTP短连接阻塞模型来模拟长连接通信，但是由于并非真实的长连接模型，
因此每30秒需要进行一次请求和数据的上下文切换，每一次切换都有引起造成一次内存浪费，从而导致服务端频繁GC。


## Nacos 2.0架构层次
Nacos 2.X 在 1.X的架构基础上 新增了对`长连接模型`的支持，
同时保留对旧客户端和openAPI的核心功能支持。

 通信层目前通过`gRPC`和`Rsocket`实现了长连接RPC调用和推送能力