## [原文](https://www.cnblogs.com/yjmyzz/p/distributed-lock-using-zookeeper.html)

## [原文](http://www.dengshenyu.com/java/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/2017/10/23/zookeeper-distributed-lock.html)

# ZooKeeper 分布式锁

　目前分布式锁，比较成熟、主流的方案有基于redis及基于zookeeper的二种方案。

　　大体来讲，基于redis的分布式锁核心指令为SETNX，即如果目标key存在，写入缓存失败返回0，反之如果目标key不存在，写入缓存成功返回1，
通过区分这二个不同的返回值，可以认为SETNX成功即为获得了锁。

　　redis分布式锁，看上去很简单，但其实要考虑周全，并不容易，
网上有一篇文章讨论得很详细：http://blog.csdn.net/ugg/article/details/41894947/，有兴趣的可以阅读一下。

　　其主要问题在于某些异常情况下，锁的释放会有问题，比如SETNX成功，应用获得锁，这时出于某种原因，比如网络中断，或程序出异常退出，
会导致锁无法及时释放，只能依赖于缓存的过期时间，但是过期时间这个值设置多大，也是一个纠结的问题，设置小了，应用处理逻辑很复杂的话，
可能会导致锁提前释放，如果设置大了，又会导致锁不能及时释放，所以那篇文章中针对这些细节讨论了很多。

　　而基于zk的分布式锁，在锁的释放问题上处理起来要容易一些，其大体思路是利用zk的“临时顺序”节点，需要获取锁时，
在某个约定节点下注册一个临时顺序节点，然后将所有临时节点按小从到大排序，如果自己注册的临时节点正好是最小的，
表示获得了锁。(zk能保证临时节点序号始终递增，所以如果后面有其它应用也注册了临时节点，序号肯定比获取锁的应用更大）

　　当应用处理完成，或者处理过程中出现某种原因，导致与zk断开，超过时间阈值（可配置）后，zk server端会自动删除该临时节点，
即：锁被释放。所有参与锁竞争的应用，只要监听父路径的子节点变化即可，有变化时（即：有应用断开或注册时），
开始抢锁，抢完了大家都在一边等着，直到有新变化时，开始新一轮抢锁。


个人感觉：zk做分布式锁机制更完善，但zk抗并发的能力弱于redis，性能上略差，
建议如果并发要求高，锁竞争激烈，可考虑用redis，如果抢锁的频度不高，用zk更适合。


## 如何使用zookeeper实现分布式锁？
在描述算法流程之前，先看下zookeeper中几个关于节点的有趣的性质：

- 有序节点：假如当前有一个父节点为/lock，我们可以在这个父节点下面创建子节点；zookeeper提供了一个可选的有序特性，
例如我们可以创建子节点“/lock/node-”并且指明有序，那么zookeeper在生成子节点时会根据当前的子节点数量自动添加整数序号，
也就是说如果是第一个创建的子节点，那么生成的子节点为/lock/node-0000000000，下一个节点则为/lock/node-0000000001，依次类推。

- 临时节点：客户端可以建立一个临时节点，在会话结束或者会话超时后，zookeeper会自动删除该节点。

- 事件监听：在读取数据时，我们可以同时对节点设置事件监听，当节点数据或结构变化时，zookeeper会通知客户端。
当前zookeeper有如下四种事件：1）节点创建；2）节点删除；3）节点数据修改；4）子节点变更。

下面描述使用zookeeper实现分布式锁的算法流程，假设锁空间的根节点为/lock：

- 客户端连接zookeeper，并在/lock下创建临时的且有序的子节点，第一个客户端对应的子节点为/lock/lock-0000000000，
第二个为/lock/lock-0000000001，以此类推。

- 客户端获取/lock下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，
如果是则认为获得锁，否则监听/lock的子节点变更消息，获得子节点变更通知后重复此步骤直至获得锁；

- 执行业务代码；

- 完成业务流程后，删除对应的子节点释放锁。

步骤1中创建的临时节点能够保证在故障的情况下锁也能被释放，考虑这么个场景：假如客户端a当前创建的子节点为序号最小的节点，
获得锁之后客户端所在机器宕机了，客户端没有主动删除子节点；如果创建的是永久的节点，那么这个锁永远不会释放，导致死锁；
由于创建的是临时节点，客户端宕机后，过了一定时间zookeeper没有收到客户端的心跳包判断会话失效，将临时节点删除从而释放锁。

另外细心的朋友可能会想到，在步骤2中获取子节点列表与设置监听这两步操作的原子性问题，
考虑这么个场景：客户端a对应子节点为/lock/lock-0000000000，客户端b对应子节点为/lock/lock-0000000001，
客户端b获取子节点列表时发现自己不是序号最小的，但是在设置监听器前客户端a完成业务流程删除了子节点/lock/lock-0000000000，
客户端b设置的监听器岂不是丢失了这个事件从而导致永远等待了？这个问题不存在的。
因为zookeeper提供的API中设置监听器的操作与读操作是原子执行的，也就是说在读子节点列表时同时设置监听器，保证不会丢失事件。

最后，对于这个算法有个极大的优化点：假如当前有1000个节点在等待锁，如果获得锁的客户端释放锁时，这1000个客户端都会被唤醒，
这种情况称为“羊群效应”；在这种羊群效应中，zookeeper需要通知1000个客户端，这会阻塞其他的操作，最好的情况应该只唤醒新的最小节点对应的客户端。
应该怎么做呢？   
在设置事件监听时，每个客户端应该对刚好在它之前的子节点设置事件监听，例如子节点列表为/lock/lock-0000000000、/lock/lock-0000000001、
/lock/lock-0000000002，序号为1的客户端监听序号为0的子节点删除消息，序号为2的监听序号为1的子节点删除消息。

所以调整后的分布式锁算法流程如下：


- 客户端连接zookeeper，并在/lock下创建临时的且有序的子节点，第一个客户端对应的子节点为/lock/lock-0000000000，
第二个为/lock/lock-0000000001，以此类推。

- 客户端获取/lock下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则认为获得锁，
否则监听刚好在自己之前一位的子节点删除消息，获得子节点变更通知后重复此步骤直至获得锁；

- 执行业务代码；

- 完成业务流程后，删除对应的子节点释放锁。

## Curator的
虽然zookeeper原生客户端暴露的API已经非常简洁了，
但是实现一个分布式锁还是比较麻烦的…我们可以直接使用curator这个开源项目提供的zookeeper分布式锁实现

```groovy
    //  Curator的确是足够牛逼，不仅封装了Zookeeper的常用API，也包装了很多常用Case的实现
    compile group: 'org.apache.curator', name: 'curator-recipes', version: '4.2.0'
```

## [自己实现 ZooKeeper 分布式锁 例子](/space/pankui/exmaple/lock/ZookDistributedLockExample.java)

